{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de5d11b-6b41-42d7-a229-0f6448b47b42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.7.0\n",
      "google-cloud-aiplatform==1.44.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24171ab7-0d8f-4fcf-8f1d-99e7c77b53fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, ClassificationMetrics\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd328827-dca8-41cc-bdd5-0cd299aa1a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bd1aa03-18c5-4323-ba9e-3ecd223169e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'kubeflow-mlops-410520' # replace with project ID\n",
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'vertex-pipelines'\n",
    "SERIES = 'dev'\n",
    "\n",
    "# gcs bucket\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-bucket\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b1f8f5-e234-4de7-ab23-067fd0ed1c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc27eafb-731e-4966-9ce3-b7c2bba05f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6683c9-d799-4cfb-b3e9-1cc023e5d030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.8', \n",
    "    packages_to_install=[\n",
    "        \"pandas==1.3.4\",\n",
    "        \"scikit-learn==1.0.1\",\n",
    "        \"google-cloud-bigquery==3.13.0\",\n",
    "        \"db-dtypes==1.1.1\"\n",
    "    ],\n",
    ")\n",
    "def get_data(\n",
    "    project_id: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    ") -> None:\n",
    "    \n",
    "    \"\"\" Loads data from BigQuery, splits it into training and test sets,\n",
    "    and saves them as CSV files.\n",
    "\n",
    "    Args:\n",
    "        project_id: str\n",
    "        dataset_train: Output[Dataset] for the training set.\n",
    "        dataset_test: Output[Dataset] for the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project= project_id)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query = \"\"\"\n",
    "\n",
    "        SELECT\n",
    "      * EXCEPT(fullVisitorId)\n",
    "    FROM\n",
    "\n",
    "      # features\n",
    "      (SELECT\n",
    "        fullVisitorId,\n",
    "        IFNULL(totals.bounces, 0) AS bounces,\n",
    "        IFNULL(totals.timeOnSite, 0) AS time_on_site\n",
    "      FROM\n",
    "        `data-to-insights.ecommerce.web_analytics`\n",
    "      WHERE\n",
    "        totals.newVisits = 1\n",
    "        AND date BETWEEN '20160801' AND '20170430') # train on first 9 months\n",
    "      JOIN\n",
    "      (SELECT\n",
    "        fullvisitorid,\n",
    "        IF(COUNTIF(totals.transactions > 0 AND totals.newVisits IS NULL) > 0, 1, 0) AS will_buy_on_return_visit\n",
    "      FROM\n",
    "          `data-to-insights.ecommerce.web_analytics`\n",
    "      GROUP BY fullvisitorid)\n",
    "      USING (fullVisitorId)\n",
    "      LIMIT 10000\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    df = query_job.to_dataframe()\n",
    "    \n",
    "    # Split Data\n",
    "    train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Save to Outputs\n",
    "    train.to_csv(dataset_train.path, index=False)\n",
    "    test.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adaa085-aa3c-4852-975b-5e5a8ce0d5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.8', \n",
    "    packages_to_install=[\n",
    "        \"xgboost==1.6.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"joblib==1.1.0\",        \n",
    "        \"scikit-learn==1.0.2\",\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model], \n",
    ") -> None:\n",
    "\n",
    "    \"\"\"Trains an XGBoost classifier on a given dataset and saves the model artifact.\n",
    "\n",
    "    Args:\n",
    "        dataset: Input[Dataset]\n",
    "            The training dataset as a Kubeflow component input.\n",
    "        model_artifact: Output[Model]\n",
    "            A Kubeflow component output for saving the trained model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            This function doesn't have a return value; its primary purpose is to produce a model artifact.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Load Training Data\n",
    "    data = pd.read_csv(dataset.path)\n",
    "\n",
    "    # Train XGBoost Model\n",
    "    model = XGBClassifier(objective=\"binary:logistic\")\n",
    "    model.fit(data.drop(columns=[\"will_buy_on_return_visit\"]), data.will_buy_on_return_visit)\n",
    "\n",
    "    # Evaluate and Log Metrics\n",
    "    score = model.score(data.drop(columns=[\"will_buy_on_return_visit\"]), data.will_buy_on_return_visit)\n",
    "\n",
    "    # Save the Model Artifact\n",
    "    os.makedirs(model_artifact.path, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_artifact.path, \"model.joblib\"))\n",
    "\n",
    "    # Metadata for the Artifact\n",
    "    model_artifact.metadata[\"train_score\"] = float(score)\n",
    "    model_artifact.metadata[\"framework\"] = \"XGBoost\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d528f4-21c9-4f74-94de-53197500b266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.8', \n",
    "    packages_to_install=[\n",
    "        \"xgboost==1.6.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"joblib==1.1.0\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "        \"google-cloud-storage==2.13.0\",\n",
    "    ],\n",
    ")\n",
    "def eval_model(\n",
    "    test_set: Input[Dataset],\n",
    "    xgb_model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    smetrics: Output[Metrics],\n",
    "    bucket_name: str,\n",
    "    score_threshold: float = 0.8,\n",
    ") -> NamedTuple(\"Outputs\", [(\"deploy\", str)]):\n",
    "    \n",
    "    \n",
    "    \"\"\"Evaluates an XGBoost model on a test dataset, logs metrics, and decides whether to deploy.\n",
    "\n",
    "    Args:\n",
    "        test_set: Input[Dataset]\n",
    "            The test dataset as a Kubeflow component input.\n",
    "        xgb_model: Input[Model]\n",
    "            The trained XGBoost model as a Kubeflow component input.\n",
    "        metrics: Output[ClassificationMetrics]\n",
    "            A Kubeflow component output for logging classification metrics.\n",
    "        smetrics: Output[Metrics]\n",
    "            A Kubeflow component output for logging scalar metrics.\n",
    "        bucket_name: str\n",
    "            The name of the Google Cloud Storage bucket containing the model.\n",
    "        score_threshold: float, default=0.8\n",
    "            The minimum score required for deployment.\n",
    "\n",
    "    Returns:\n",
    "        NamedTuple(\"Outputs\", [(\"deploy\", str)])\n",
    "            A named tuple with a single field:\n",
    "            * deploy: str\n",
    "                A string indicating whether to deploy the model (\"true\" or \"false\").\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import storage\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix\n",
    "    from collections import namedtuple\n",
    "\n",
    "\n",
    "    # ----- 1. Load Test Data and Model -----\n",
    "    data = pd.read_csv(test_set.path)\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob_path = xgb_model.uri.replace(f\"gs://{bucket_name}/\", \"\")\n",
    "    smetrics.log_metric(\"blob_path\", str(blob_path))\n",
    "\n",
    "    blob = bucket.blob(f\"{blob_path}/model.joblib\")\n",
    "    with blob.open(mode=\"rb\") as file:\n",
    "        model = joblib.load(file)\n",
    "\n",
    "    # ----- 2. Evaluation and Metrics -----\n",
    "    y_scores = model.predict_proba(data.drop(columns=[\"will_buy_on_return_visit\"]))[:, 1]\n",
    "    y_pred = model.predict(data.drop(columns=[\"will_buy_on_return_visit\"]))\n",
    "    score = model.score(data.drop(columns=[\"will_buy_on_return_visit\"]), data.will_buy_on_return_visit)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(data.will_buy_on_return_visit.to_numpy(), y_scores, pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(),thresholds.tolist())\n",
    "\n",
    "    cm = confusion_matrix(data.will_buy_on_return_visit, y_pred)\n",
    "    metrics.log_confusion_matrix([\"False\", \"True\"], cm.tolist())\n",
    "    smetrics.log_metric(\"score\", float(score))\n",
    "\n",
    "    # ----- 3. Deployment Decision Logic -----\n",
    "    deploy = \"true\" if score >= score_threshold else \"false\"\n",
    "\n",
    "    # ----- 4. Metadata Update -----\n",
    "    xgb_model.metadata[\"test_score\"] = float(score)\n",
    "\n",
    "    Outputs = namedtuple(\"Outputs\", [\"deploy\"])\n",
    "    return Outputs(deploy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7444d1-93e8-45aa-8b7b-2674a7202515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.8', \n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n",
    ")\n",
    "def deploy_xgboost_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    ") -> None:\n",
    "    \"\"\"Deploys an XGBoost model to Vertex AI Endpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to deploy.\n",
    "        project_id: The Google Cloud project ID.\n",
    "        vertex_endpoint: Output[Artifact] representing the deployed Vertex AI Endpoint.\n",
    "        vertex_model: Output[Model] representing the deployed Vertex AI Model.\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # Initialize AI Platform with project \n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    # Upload the Model\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"xgb-classification\",\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\",\n",
    "    )\n",
    "\n",
    "    # Deploy the Model to an Endpoint\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    # Save Outputs\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "194b4eab-36ae-4e88-a16f-65d74f0bb3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-bucket\"\n",
    "PIPELINE_ROOT = BUCKET_NAME + \"/pipeline_root/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947dd179-3661-4223-8c83-b3b7a8ba5d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT + \"xgboost-pipeline-v2\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"xgboost-pipeline-with-deployment-v2\",\n",
    ")\n",
    "def pipeline():\n",
    "    \"\"\"\n",
    "    Defines steps in pipeline\n",
    "    \"\"\"\n",
    "    dataset_op = get_data(project_id = PROJECT_ID)\n",
    "    training_op = train_model(dataset = dataset_op.outputs[\"dataset_train\"])\n",
    "    eval_op = eval_model(\n",
    "        test_set=dataset_op.outputs[\"dataset_test\"],\n",
    "        xgb_model=training_op.outputs[\"model_artifact\"],\n",
    "        bucket_name = \"kubeflow-mlops-410520-bucket\"\n",
    "    )\n",
    "\n",
    "    with dsl.If(\n",
    "        eval_op.outputs[\"deploy\"] == \"true\",\n",
    "        name=\"deploy\",\n",
    "    ):\n",
    "\n",
    "        deploy_op = deploy_xgboost_model(model = training_op.outputs[\"model_artifact\"],\n",
    "                         project_id = PROJECT_ID,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55eb3abe-672c-4352-afb0-ee0d008fa0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b639c-ecff-40a6-92a2-8a52f3926c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/990864364836/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-v2-20240614133146\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/990864364836/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-v2-20240614133146')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgboost-pipeline-with-deployment-v2-20240614133146?project=990864364836\n",
      "PipelineJob projects/990864364836/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-v2-20240614133146 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"demo-pipeline\",\n",
    "    template_path=\"pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a756dc-fed0-42a0-a2a6-c431e3db3705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
